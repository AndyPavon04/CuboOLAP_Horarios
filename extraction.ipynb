{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements: pdfplumber, pandas, tabula-py , numpy\n",
    "import re\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# SQLAlchemy + pyodbc para SQL Server\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Connection\n",
    "import pyodbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDFS = [\n",
    "    \"pdfs/PA_OTO√ëO_2025_SEMESTRAL_ICC.pdf\",\n",
    "    \"pdfs/PA_OTO√ëO_2025_SEMESTRAL_ITI.pdf\",\n",
    "    \"pdfs/PA_OTO√ëO_2025_SEMESTRAL_LCC.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_header(cols):\n",
    "    return [re.sub(r\"\\s+\", \" \", c).strip().lower() for c in cols]\n",
    "\n",
    "def extract_tables_pdfplumber(pdf_path):\n",
    "    rows = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            for table in page.extract_tables():\n",
    "                if not table or len(table) < 2: \n",
    "                    continue\n",
    "                header = clean_header(table[0])\n",
    "                # heur√≠stica: columnas esperadas\n",
    "                if {\"nrc\",\"clave\",\"materia\",\"d√≠as\",\"hora\",\"profesor\",\"sal√≥n\"}.issubset(set(header)) or \\\n",
    "                   {\"nrc\",\"clave\",\"materia\",\"dias\",\"hora\",\"profesor\",\"salon\"}.issubset(set(header)):\n",
    "                    for r in table[1:]:\n",
    "                        if r and any(x for x in r):\n",
    "                            rows.append(dict(zip(header, r)))\n",
    "    return pd.DataFrame(rows) if rows else pd.DataFrame()\n",
    "\n",
    "def extract_all():\n",
    "    frames = []\n",
    "    for p in PDFS:\n",
    "        if Path(p).exists():\n",
    "            df = extract_tables_pdfplumber(p)\n",
    "            if not df.empty:\n",
    "                df[\"origen_pdf\"] = Path(p).name\n",
    "                frames.append(df)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "raw = extract_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizaci√≥n de encabezados frecuentes\n",
    "raw = raw.rename(columns={\n",
    "    \"dias\": \"d√≠as\", \"salon\": \"sal√≥n\"\n",
    "})\n",
    "\n",
    "# Limpieza de profesor\n",
    "def normalizar_profesor(x: str):\n",
    "    if not isinstance(x, str):\n",
    "        return None\n",
    "    x = re.sub(r\"\\s+\", \" \", x).strip()\n",
    "    x = x.replace(\" - \", \" \")\n",
    "    return x.title()\n",
    "\n",
    "raw[\"profesor\"] = raw[\"profesor\"].apply(normalizar_profesor)\n",
    "\n",
    "# Pasar todo a min√∫sculas y eliminar espacios\n",
    "raw.columns = [c.strip().lower() for c in raw.columns]\n",
    "\n",
    "# Asegurar que exista la columna 'hora'\n",
    "if \"horario\" in raw.columns and \"hora\" not in raw.columns:\n",
    "    raw.rename(columns={\"horario\": \"hora\"}, inplace=True)\n",
    "elif \"hora \" in raw.columns:\n",
    "    raw.rename(columns={\"hora \": \"hora\"}, inplace=True)\n",
    "elif \"hora\\n\" in raw.columns:\n",
    "    raw.rename(columns={\"hora\\n\": \"hora\"}, inplace=True)\n",
    "elif \"h\" in raw.columns:  # casos raros de extracci√≥n truncada\n",
    "    raw.rename(columns={\"h\": \"hora\"}, inplace=True)\n",
    "\n",
    "# Si sigue sin existir, crear una columna vac√≠a para evitar errores posteriores\n",
    "if \"hora\" not in raw.columns:\n",
    "    raw[\"hora\"] = None\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Funci√≥n para parsear horas de forma robusta\n",
    "# ---------------------------------------------------------\n",
    "def parse_hora(rango):\n",
    "    \"\"\"Parses hour ranges like '0700-0859', '07:00-08:59', '7:00 - 8:59'.\"\"\"\n",
    "    if not isinstance(rango, str):\n",
    "        return pd.Series([None, None, None])\n",
    "    \n",
    "    s = rango.strip()\n",
    "    s = re.sub(r\"\\s+\", \"\", s)\n",
    "    if s.lower() in [\"nan\", \"none\", \"\"]:\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "    patron = r\"(\\d{1,2}):?(\\d{2})-(\\d{1,2}):?(\\d{2})\"\n",
    "    m = re.match(patron, s)\n",
    "    if not m:\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "    h1, m1, h2, m2 = map(int, m.groups())\n",
    "    start = pd.to_datetime(f\"{h1:02d}:{m1:02d}\", format=\"%H:%M\", errors=\"coerce\")\n",
    "    end   = pd.to_datetime(f\"{h2:02d}:{m2:02d}\", format=\"%H:%M\", errors=\"coerce\")\n",
    "\n",
    "    if pd.isna(start) or pd.isna(end):\n",
    "        return pd.Series([None, None, None])\n",
    "    duracion = int((end - start).total_seconds() / 60)\n",
    "    if duracion <= 0:\n",
    "        return pd.Series([None, None, None])\n",
    "\n",
    "    return pd.Series([start.time(), end.time(), duracion])\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Aplicar la funci√≥n y crear las tres columnas\n",
    "# ---------------------------------------------------------\n",
    "raw[[\"hora_inicio\", \"hora_fin\", \"duracion_min\"]] = raw[\"hora\"].apply(parse_hora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIA_MAP = {\"L\":\"Lunes\",\"A\":\"Martes\",\"M\":\"Miercoles\",\n",
    "           \"J\":\"Jueves\",\"V\":\"Viernes\",\"S\":\"S√°bado\"}\n",
    "\n",
    "# Expandir por m√∫ltiples d√≠as en una sola fila (si aplica)\n",
    "def explotar_por_dia(df):\n",
    "    out = []\n",
    "    for _, row in df.iterrows():\n",
    "        dias = str(row[\"d√≠as\"]).replace(\" \", \"\")\n",
    "        if \",\" in dias:\n",
    "            tokens = dias.split(\",\")\n",
    "        else:\n",
    "            tokens = list(dias)  # \"AJL\" -> [\"A\",\"J\",\"L\"]\n",
    "        for d in tokens:\n",
    "            r = row.copy()\n",
    "            r[\"dia_codigo\"] = d\n",
    "            r[\"dia_semana\"] = DIA_MAP.get(d, d)\n",
    "            out.append(r)\n",
    "    return pd.DataFrame(out)\n",
    "\n",
    "curated = explotar_por_dia(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas sin id_materia: 0\n"
     ]
    }
   ],
   "source": [
    "# Sal√≥n -> edificio/aula: \"1CCO4/203\" -> edificio=1CCO4, aula=203\n",
    "def split_salon(s):\n",
    "    if not isinstance(s, str): return pd.Series([None, None, None])\n",
    "    s = s.strip()\n",
    "    m = re.match(r\"([^/]+)/?(\\w+)?\", s)\n",
    "    if not m: return pd.Series([s, None, s])\n",
    "    edificio, aula = m.group(1), m.group(2)\n",
    "    return pd.Series([edificio, aula, s])\n",
    "\n",
    "curated[[\"edificio\",\"aula\",\"codigo_salon\"]] = curated[\"sal√≥n\"].apply(split_salon)\n",
    "\n",
    "# Dimensiones (surrogate keys)\n",
    "def build_dim(df, col_key, cols_keep, start_id=1, name_id=\"id\"):\n",
    "    d = df[cols_keep].drop_duplicates().reset_index(drop=True)\n",
    "    d.insert(0, name_id, range(start_id, start_id+len(d)))\n",
    "    return d\n",
    "\n",
    "dim_docente = build_dim(curated, \"profesor\", [\"profesor\"], name_id=\"id_docente\")\n",
    "dim_materia = build_dim(curated, \"materia\", [\"clave\",\"materia\"], name_id=\"id_materia\")\n",
    "dim_espacio = build_dim(curated, \"codigo_salon\", [\"edificio\",\"aula\",\"codigo_salon\"], name_id=\"id_espacio\")\n",
    "\n",
    "# dim_tiempo por fila (d√≠a + rango)\n",
    "dim_tiempo = curated[[\"dia_codigo\",\"dia_semana\",\"hora_inicio\",\"hora_fin\"]].drop_duplicates().reset_index(drop=True)\n",
    "dim_tiempo.insert(0, \"id_tiempo\", range(1, len(dim_tiempo)+1))\n",
    "\n",
    "# Hechos (join a dimensiones)\n",
    "def map_id(df, dim, key_cols_df, key_cols_dim, id_col):\n",
    "    if isinstance(key_cols_df, str):\n",
    "        key_cols_df = [key_cols_df]\n",
    "    if isinstance(key_cols_dim, str):\n",
    "        key_cols_dim = [key_cols_dim]\n",
    "\n",
    "    df[\"_key_\"] = df[key_cols_df].astype(str).agg(\"|\".join, axis=1)\n",
    "    dim[\"_key_\"] = dim[key_cols_dim].astype(str).agg(\"|\".join, axis=1)\n",
    "\n",
    "    merged = df.merge(dim[[\"_key_\", id_col]], on=\"_key_\", how=\"left\", validate=\"m:1\")\n",
    "    result = merged[id_col].values\n",
    "\n",
    "    df.drop(columns=\"_key_\", inplace=True, errors=\"ignore\")\n",
    "    dim.drop(columns=\"_key_\", inplace=True, errors=\"ignore\")\n",
    "\n",
    "    return result\n",
    "\n",
    "hechos = curated.copy()\n",
    "hechos[\"id_docente\"] = map_id(hechos, dim_docente, \"profesor\", \"profesor\", \"id_docente\")\n",
    "hechos[\"id_materia\"] = map_id(hechos, dim_materia, [\"clave\",\"materia\"], [\"clave\",\"nombre_materia\" if \"nombre_materia\" in dim_materia.columns else \"materia\"], \"id_materia\")\n",
    "hechos[\"id_espacio\"] = map_id(hechos, dim_espacio, \"codigo_salon\", \"codigo_salon\", \"id_espacio\")\n",
    "hechos = hechos.merge(dim_tiempo, on=[\"dia_codigo\",\"dia_semana\",\"hora_inicio\",\"hora_fin\"], how=\"left\")\n",
    "\n",
    "print(\"Filas sin id_materia:\", hechos[\"id_materia\"].isna().sum())\n",
    "\n",
    "hechos_clase = hechos[[\n",
    "    \"id_docente\"    ,\"id_materia\",\"id_espacio\",\"id_tiempo\",\n",
    "    \"nrc\",\"clave\",\"secc\" if \"secc\" in hechos.columns else \"secci√≥n\" if \"secci√≥n\" in hechos.columns else \"d√≠as\",\n",
    "    \"duracion_min\"\n",
    "]].rename(columns=lambda c: {\"d√≠as\":\"seccion\"}.get(c, c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Archivos CSV guardados correctamente en la carpeta 'data_export'\n"
     ]
    }
   ],
   "source": [
    "#-------------------------------------------------------------\n",
    "# Guardar los resultados intermedios en archivos CSV\n",
    "#-------------------------------------------------------------\n",
    "output_dir = Path(\"data_export\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "#print(\">> Guardando archivos CSV intermedios en ./data_export/\")\n",
    "\n",
    "raw.to_csv(output_dir / \"data_raw.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "curated.to_csv(output_dir / \"data_curated.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "hechos_clase.to_csv(output_dir / \"data_hechos.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_docente.to_csv(output_dir / \"dim_docente.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_materia.to_csv(output_dir / \"dim_materia.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_espacio.to_csv(output_dir / \"dim_espacio.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "dim_tiempo.to_csv(output_dir / \"dim_tiempo.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\">> Archivos CSV guardados correctamente en la carpeta 'data_export'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Limpieza y validaci√≥n robusta de horas antes de la carga\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Normaliza tipos de hora\n",
    "raw[\"hora_inicio\"] = pd.to_datetime(raw[\"hora_inicio\"], errors=\"coerce\").dt.time\n",
    "raw[\"hora_fin\"] = pd.to_datetime(raw[\"hora_fin\"], errors=\"coerce\").dt.time\n",
    "\n",
    "# Quita filas sin hora v√°lida\n",
    "raw = raw.dropna(subset=[\"hora_inicio\", \"hora_fin\"]).reset_index(drop=True)\n",
    "\n",
    "# Filtra filas donde hora_fin <= hora_inicio\n",
    "def es_valida(row):\n",
    "    try:\n",
    "        return row[\"hora_fin\"] > row[\"hora_inicio\"]\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "mask_validas = raw.apply(es_valida, axis=1)\n",
    "raw = raw.loc[mask_validas].copy().reset_index(drop=True)\n",
    "\n",
    "# Recalcula duraci√≥n por consistencia\n",
    "def calcular_duracion(row):\n",
    "    try:\n",
    "        start = pd.to_datetime(str(row[\"hora_inicio\"]), format=\"%H:%M:%S\")\n",
    "        end = pd.to_datetime(str(row[\"hora_fin\"]), format=\"%H:%M:%S\")\n",
    "        dur = int((end - start).total_seconds() / 60)\n",
    "        return dur if dur > 0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "raw[\"duracion_min\"] = raw.apply(calcular_duracion, axis=1)\n",
    "raw = raw.dropna(subset=[\"duracion_min\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Conexi√≥n a SQL Server establecida correctamente (Windows Auth).\n",
      ">> Eliminando tablas existentes...\n",
      "‚úÖ Tablas eliminadas correctamente.\n",
      ">> Creando tablas...\n",
      "‚úÖ Tablas creadas correctamente.\n",
      "NaN limpiados en dim_docente\n",
      "NaN limpiados en dim_materia\n",
      "NaN limpiados en dim_espacio\n",
      "NaN limpiados en dim_tiempo\n",
      "NaN limpiados en hechos_clase\n",
      ">> Limpiando tablas con TRUNCATE...\n",
      "‚úÖ Tablas limpiadas correctamente con TRUNCATE.\n",
      "‚úÖ 105 filas insertadas en dim_docente\n",
      "‚úÖ 96 filas insertadas en dim_materia\n",
      "‚úÖ 42 filas insertadas en dim_espacio\n",
      "‚úÖ 47 filas insertadas en dim_tiempo\n",
      "‚úÖ 1184 filas insertadas en hechos_clase\n",
      ">> Creando Claves For√°neas...\n",
      "‚úÖ Claves For√°neas creadas exitosamente.\n",
      "‚úÖ Todos los datos cargados exitosamente en SQL Server.\n",
      "üîö Conexi√≥n a SQL Server cerrada.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------------\n",
    "# üîß CONFIGURACI√ìN GLOBAL\n",
    "# ---------------------------------------------------------------------------\n",
    "SERVER_NAME = \"ANDYPAVON\" \n",
    "DATABASE_NAME = \"horariosCubo\" \n",
    "\n",
    "# Cadena de conexi√≥n\n",
    "CONNECTION_STRING = (\n",
    "    f\"mssql+pyodbc:///?odbc_connect=\"\n",
    "    f\"Driver={{ODBC Driver 17 for SQL Server}};\" \n",
    "    f\"Server={SERVER_NAME};\"\n",
    "    f\"Database={DATABASE_NAME};\"\n",
    "    f\"Trusted_Connection=yes;\" \n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üîó CONEXI√ìN A SQL SERVER\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    engine = create_engine(CONNECTION_STRING)\n",
    "    conn = engine.connect()\n",
    "    conn.execute(text(\"SELECT 1\")) \n",
    "    print(\"‚úÖ Conexi√≥n a SQL Server establecida correctamente (Windows Auth).\")\n",
    "\n",
    "except Exception as err:\n",
    "    print(f\"‚ùå Error al conectar a SQL Server (Verifica el driver ODBC): {err}\")\n",
    "    raise SystemExit()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üß± CREACI√ìN DE TABLAS (DDL SIN CLAVES FOR√ÅNEAS INICIALES)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# PASO 1: ELIMINAR TABLAS EN ORDEN INVERSO DE DEPENDENCIA\n",
    "drop_statements = [\n",
    "    \"\"\"\n",
    "    IF OBJECT_ID('hechos_clase', 'U') IS NOT NULL DROP TABLE hechos_clase;\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    IF OBJECT_ID('dim_docente', 'U') IS NOT NULL DROP TABLE dim_docente;\n",
    "    IF OBJECT_ID('dim_materia', 'U') IS NOT NULL DROP TABLE dim_materia;\n",
    "    IF OBJECT_ID('dim_espacio', 'U') IS NOT NULL DROP TABLE dim_espacio;\n",
    "    IF OBJECT_ID('dim_tiempo', 'U') IS NOT NULL DROP TABLE dim_tiempo;\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "print(\">> Eliminando tablas existentes...\")\n",
    "for ddl in drop_statements:\n",
    "    conn.execute(text(ddl))\n",
    "conn.commit()\n",
    "print(\"‚úÖ Tablas eliminadas correctamente.\")\n",
    "\n",
    "\n",
    "# PASO 2: CREAR TABLAS (Dimensiones antes que Hechos, sin FKs en Hechos)\n",
    "ddl_statements = [\n",
    "    \"\"\"\n",
    "    CREATE TABLE dim_docente (id_docente INT PRIMARY KEY, nombre_completo NVARCHAR(200))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE dim_materia (id_materia INT PRIMARY KEY, clave VARCHAR(50), nombre_materia NVARCHAR(200))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE dim_espacio (id_espacio INT PRIMARY KEY, edificio VARCHAR(50), aula VARCHAR(50), codigo_salon VARCHAR(100))\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE dim_tiempo (id_tiempo INT PRIMARY KEY, dia_codigo VARCHAR(10), dia_semana NVARCHAR(20), hora_inicio TIME, hora_fin TIME)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    CREATE TABLE hechos_clase (\n",
    "        id_hecho INT IDENTITY(1,1) PRIMARY KEY, \n",
    "        id_docente INT, id_materia INT, id_espacio INT, id_tiempo INT,\n",
    "        nrc VARCHAR(20), clave VARCHAR(50), seccion VARCHAR(50), duracion_min INT\n",
    "        -- Las FKs se a√±adir√°n al final\n",
    "    )\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\">> Creando tablas...\")\n",
    "for ddl in ddl_statements:\n",
    "    conn.execute(text(ddl))\n",
    "conn.commit()\n",
    "print(\"‚úÖ Tablas creadas correctamente.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üßπ Limpieza y normalizaci√≥n (Se mantiene el c√≥digo anterior)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# Renombrar columnas si existen\n",
    "if \"profesor\" in dim_docente.columns:\n",
    "    dim_docente.rename(columns={\"profesor\": \"nombre_completo\"}, inplace=True)\n",
    "if \"materia\" in dim_materia.columns:\n",
    "    dim_materia.rename(columns={\"materia\": \"nombre_materia\"}, inplace=True)\n",
    "\n",
    "# Normalizar nombre de columna 'secc' -> 'seccion'\n",
    "if \"secc\" in hechos_clase.columns:\n",
    "    hechos_clase.rename(columns={\"secc\": \"seccion\"}, inplace=True)\n",
    "elif \"secci√≥n\" in hechos_clase.columns:\n",
    "    hechos_clase.rename(columns={\"secci√≥n\": \"seccion\"}, inplace=True)\n",
    "elif \"d√≠as\" in hechos_clase.columns:\n",
    "    hechos_clase.rename(columns={\"d√≠as\": \"seccion\"}, inplace=True)\n",
    "\n",
    "# Reemplazar NaN y valores \"nan\" o \"NaT\" por None\n",
    "def clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.replace({np.nan: None, \"nan\": None, \"NaN\": None, \"NaT\": None})\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == object:\n",
    "            df[col] = df[col].astype(str).replace(\n",
    "                {\"nan\": None, \"None\": None, \"NaN\": None, \"NaT\": None, \"\": None}\n",
    "            )\n",
    "    return df\n",
    "\n",
    "for df_name, df in {\n",
    "    \"dim_docente\": dim_docente, \"dim_materia\": dim_materia,\n",
    "    \"dim_espacio\": dim_espacio, \"dim_tiempo\": dim_tiempo,\n",
    "    \"hechos_clase\": hechos_clase,\n",
    "}.items():\n",
    "    globals()[df_name] = clean_dataframe(df)\n",
    "    print(f\"NaN limpiados en {df_name}\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üßπ Limpieza previa en base de datos (TRUNCATE ahora funciona sin FKs)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\">> Limpiando tablas con TRUNCATE...\")\n",
    "truncate_statements = [\n",
    "    \"TRUNCATE TABLE hechos_clase\",\n",
    "    \"TRUNCATE TABLE dim_docente\",\n",
    "    \"TRUNCATE TABLE dim_materia\",\n",
    "    \"TRUNCATE TABLE dim_espacio\",\n",
    "    \"TRUNCATE TABLE dim_tiempo\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with engine.begin() as t_conn:\n",
    "        for stmt in truncate_statements:\n",
    "            t_conn.execute(text(stmt))\n",
    "        t_conn.commit()\n",
    "    print(\"‚úÖ Tablas limpiadas correctamente con TRUNCATE.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR al truncar tablas: {e}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üß† FUNCI√ìN DE INSERCI√ìN CORREGIDA (Usamos 'append' para todo despu√©s del TRUNCATE)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def insert_dataframe_to_sql_server(df: pd.DataFrame, table_name: str, conn: Connection):\n",
    "    if df.empty:\n",
    "        print(f\"(‚ö†Ô∏è {table_name} est√° vac√≠o, no se inserta nada)\")\n",
    "        return 0 \n",
    "    \n",
    "    rows_inserted = df.to_sql(\n",
    "        name=table_name,\n",
    "        con=engine, # Usamos 'engine' para todas las operaciones\n",
    "        if_exists='append', # Usamos 'append' despu√©s del TRUNCATE\n",
    "        index=False,\n",
    "        chunksize=1000 \n",
    "    )\n",
    "    # Devolvemos el n√∫mero real de filas del DataFrame para el mensaje de √©xito\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üöÄ CARGA DE DATOS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "rows_inserted = insert_dataframe_to_sql_server(dim_docente, \"dim_docente\", engine)\n",
    "print(f\"‚úÖ {rows_inserted} filas insertadas en dim_docente\")\n",
    "\n",
    "rows_inserted = insert_dataframe_to_sql_server(dim_materia, \"dim_materia\", engine)\n",
    "print(f\"‚úÖ {rows_inserted} filas insertadas en dim_materia\")\n",
    "\n",
    "rows_inserted = insert_dataframe_to_sql_server(dim_espacio, \"dim_espacio\", engine)\n",
    "print(f\"‚úÖ {rows_inserted} filas insertadas en dim_espacio\")\n",
    "\n",
    "rows_inserted = insert_dataframe_to_sql_server(dim_tiempo, \"dim_tiempo\", engine)\n",
    "print(f\"‚úÖ {rows_inserted} filas insertadas en dim_tiempo\")\n",
    "\n",
    "\n",
    "rows_inserted = insert_dataframe_to_sql_server(hechos_clase, \"hechos_clase\", engine)\n",
    "print(f\"‚úÖ {rows_inserted} filas insertadas en hechos_clase\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üîó CREACI√ìN FINAL DE CLAVES FOR√ÅNEAS\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "print(\">> Creando Claves For√°neas...\")\n",
    "fk_statements = [\n",
    "    \"ALTER TABLE hechos_clase ADD CONSTRAINT FK_Docente FOREIGN KEY (id_docente) REFERENCES dim_docente(id_docente);\",\n",
    "    \"ALTER TABLE hechos_clase ADD CONSTRAINT FK_Materia FOREIGN KEY (id_materia) REFERENCES dim_materia(id_materia);\",\n",
    "    \"ALTER TABLE hechos_clase ADD CONSTRAINT FK_Espacio FOREIGN KEY (id_espacio) REFERENCES dim_espacio(id_espacio);\",\n",
    "    \"ALTER TABLE hechos_clase ADD CONSTRAINT FK_Tiempo FOREIGN KEY (id_tiempo) REFERENCES dim_tiempo(id_tiempo);\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    with engine.begin() as t_conn:\n",
    "        for stmt in fk_statements:\n",
    "            t_conn.execute(text(stmt))\n",
    "        t_conn.commit()\n",
    "    print(\"‚úÖ Claves For√°neas creadas exitosamente.\")\n",
    "except Exception as e:\n",
    "    # Esto atrapar√° errores si las FKs ya existen de una ejecuci√≥n previa\n",
    "    print(f\"‚ö†Ô∏è Las FKs ya existen o hubo un error al crearlas: {e}\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Todos los datos cargados exitosamente en SQL Server.\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# üîö Cierre\n",
    "# ---------------------------------------------------------------------------\n",
    "conn.close()\n",
    "print(\"üîö Conexi√≥n a SQL Server cerrada.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
